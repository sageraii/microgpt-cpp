# Ch02: 신경망 첫걸음

## 학습 목표

- 신경망이 무엇인지 직관적으로 이해한다
- 가중치(weight)가 "신경망이 아는 것"임을 파악한다
- `linear()`, `softmax()`, `rmsnorm()` 세 함수를 직접 구현한다
- 랜덤 가중치로 "다음 문자 예측"을 시도해 보고, 왜 틀리는지 이해한다

---

## 개념 설명

### 신경망이란?

신경망(Neural Network)은 **"숫자를 넣으면 숫자가 나오는 함수"** 입니다. 특별히 복잡한 것이 아닙니다. 다만 그 안에 엄청나게 많은 조절 가능한 숫자(가중치)가 들어 있어서, 학습을 통해 원하는 동작을 하도록 조율할 수 있습니다.

```
일반 함수:         신경망:
                    가중치 (조절 가능한 숫자들)
입력 → f(x) → 출력    ↓
                 입력 → [신경망] → 출력
                 [4, 12, 12, 0]   [확률들]
                 (토큰 시퀀스)    (다음 토큰 예측)
```

### 가중치란?

가중치(weight)는 신경망이 "아는 것"을 저장하는 숫자들입니다. 처음에는 완전히 랜덤한 숫자입니다. 학습을 거치면서 점점 의미 있는 값으로 바뀝니다.

```
학습 전 가중치:   [0.03, -0.12, 0.07, -0.05, ...]  ← 의미 없는 랜덤 숫자
       학습 ...
학습 후 가중치:   [0.81, -0.23, 0.14,  0.67, ...]  ← "이름 패턴"을 담은 숫자
```

마치 사람이 공부하면서 뇌의 신경 연결이 강해지듯, GPT는 학습하면서 가중치 숫자가 바뀝니다.

---

## 코드 작성

새 파일 `ch02_neural_net.py`를 만들고 한 줄씩 따라 써 보세요.

### 기본 준비

```python
# ch02_neural_net.py

import math
import random

random.seed(42)  # 재현 가능한 결과를 위해
```

---

### `linear()` 함수: 가중 투표

`linear()`는 신경망의 가장 기본 연산입니다. 입력 벡터와 가중치 행렬을 곱해서 출력 벡터를 만듭니다.

**비유: 가중 투표**

학교 반장 선거를 생각해 보세요. 세 명의 심사위원이 각 후보에게 점수를 줍니다. 각 심사위원의 영향력(가중치)이 다를 때, 최종 점수는 "가중 투표" 결과입니다.

```
심사위원 의견:  [1.0, 2.0, 3.0]
가중치 행렬:
  후보 A: [0.1, 0.2, 0.3]  → 1.0×0.1 + 2.0×0.2 + 3.0×0.3 = 0.1+0.4+0.9 = 1.4
  후보 B: [0.4, 0.5, 0.6]  → 1.0×0.4 + 2.0×0.5 + 3.0×0.6 = 0.4+1.0+1.8 = 3.2

결과: [1.4, 3.2]
```

**코드:**

```python
# microgpt.py 94~95줄과 동일한 로직 (float 버전)
def linear(x, w):
    """
    x: 입력 벡터  (길이 n인 리스트)
    w: 가중치 행렬 (m행 × n열 리스트의 리스트)
    반환: 출력 벡터 (길이 m인 리스트)
    """
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]
    #        ↑ 각 출력 원소 = 가중치 행(wo)과 입력(x)의 내적(dot product)
```

한 줄씩 분해해서 보면:

```python
# linear()를 풀어 쓴 버전 — 같은 계산을 for 루프로 표현
def linear_verbose(x, w):
    output = []
    for wo in w:              # w의 각 행(row)에 대해
        dot = 0
        for wi, xi in zip(wo, x):  # 행의 각 원소와 입력의 각 원소를 짝지어
            dot += wi * xi    # 곱한 뒤 더한다 (내적)
        output.append(dot)
    return output
```

---

### `softmax()` 함수: 점수를 확률로

신경망의 출력(logits)은 그냥 숫자들입니다. 이것을 "확률"로 바꿔야 "다음 글자가 a일 확률은 30%, b일 확률은 5%..." 식으로 해석할 수 있습니다. 이 변환을 `softmax()`가 합니다.

**비유: 점수를 퍼센트로**

```
시험 점수:    수학 80점, 영어 60점, 과학 40점
점수 합계:    80 + 60 + 40 = 180
퍼센트 변환:  수학 44%, 영어 33%, 과학 22%   ← 합이 100%
```

softmax는 이것과 비슷하지만, 점수에 `exp()`(지수 함수)를 먼저 적용해서 큰 값이 더 두드러지게 만듭니다.

**수치 안정성 — 왜 max를 빼는가?**

`exp(1000)`은 계산할 수 없을 만큼 큰 숫자가 됩니다. 이를 막기 위해 모든 값에서 최댓값을 먼저 뺍니다. 수학적으로 결과는 동일하지만 숫자가 넘치지 않습니다.

```
원래: exp([1000, 999]) / sum(exp([1000, 999]))  → 오버플로 오류!
개선: exp([1000-1000, 999-1000]) / sum(...)
    = exp([0, -1]) / sum(exp([0, -1]))          → 안전하게 계산
```

**코드:**

```python
# microgpt.py 97~101줄과 동일한 로직 (float 버전)
def softmax(logits):
    """
    logits: 임의의 실수 리스트
    반환: 합이 1.0인 확률 리스트
    """
    max_val = max(logits)                      # ① 최댓값을 구한다
    exps = [math.exp(v - max_val) for v in logits]  # ② 각 값에서 max를 빼고 exp 적용
    total = sum(exps)                          # ③ 전체 합
    return [e / total for e in exps]           # ④ 각 값을 합으로 나눠 확률로 변환
```

---

### `rmsnorm()` 함수: 벡터 정규화

신경망 안에서 숫자들이 너무 크거나 너무 작아지면 학습이 불안정해집니다. `rmsnorm()`은 벡터의 크기를 일정한 범위로 맞춰 줍니다.

**비유: 점수 범위 맞추기**

반마다 시험 난이도가 달라서 1반 평균은 90점, 2반 평균은 40점이라면 공정한 비교가 어렵습니다. 이를 모든 반이 비슷한 범위로 조정하는 것이 정규화입니다.

```
정규화 전: [100, 200, 300, 400]  ← 값의 범위가 넓다
정규화 후: [0.18, 0.37, 0.55, 0.73]  ← 비슷한 범위로 조정
```

**코드:**

```python
# microgpt.py 103~106줄과 동일한 로직 (float 버전)
def rmsnorm(x):
    """
    x: 실수 리스트
    반환: RMS(제곱평균제곱근)로 정규화된 리스트
    """
    ms = sum(xi * xi for xi in x) / len(x)  # ① 각 원소를 제곱하여 평균 (평균제곱)
    scale = (ms + 1e-5) ** -0.5             # ② 그 값의 제곱근의 역수 (나눠줄 척도)
    return [xi * scale for xi in x]         # ③ 각 원소에 척도를 곱해 정규화
    #   참고: 1e-5 는 0.00001 을 뜻합니다. 분모가 0이 되는 것을 막는 안전장치입니다.
```

RMS(Root Mean Square)를 구하는 과정:

```
입력: [3.0, 4.0]
  ① 제곱:       [9.0, 16.0]
  ② 평균:       (9.0 + 16.0) / 2 = 12.5
  ③ 제곱근:     √12.5 ≈ 3.536  ← 이것이 RMS
  ④ 역수(scale): 1 / 3.536 ≈ 0.283
  ⑤ 정규화:     [3.0 × 0.283, 4.0 × 0.283] = [0.849, 1.131]
```

---

### 종합 실습: 랜덤 가중치로 "다음 문자 예측" 해보기

이제 세 함수를 조합해서 실제로 "다음 글자 예측"을 시도해 봅니다. 가중치는 아직 학습 전이라 완전히 랜덤입니다.

```python
if __name__ == '__main__':

    # ── linear() 테스트 ──────────────────────────────────────────────
    print("=== linear() 테스트 ===")
    x = [1.0, 2.0, 3.0]
    w = [
        [0.1, 0.2, 0.3],   # 출력 0번 행
        [0.4, 0.5, 0.6],   # 출력 1번 행
    ]
    result = linear(x, w)
    print(f"입력: {x}")
    print(f"가중치:\n  {w[0]}\n  {w[1]}")
    print(f"출력: {result}")
    # 출력: [1.4000000000000001, 3.2]

    # ── softmax() 테스트 ─────────────────────────────────────────────
    print("\n=== softmax() 테스트 ===")
    logits = [2.0, 1.0, 0.5]
    probs = softmax(logits)
    print(f"입력 logits: {logits}")
    print(f"출력 확률:   {[round(p, 3) for p in probs]}")
    print(f"확률 합계:   {sum(probs):.6f}")
    # 출력: [0.659, 0.243, 0.098], 합 = 1.000000

    # ── rmsnorm() 테스트 ─────────────────────────────────────────────
    print("\n=== rmsnorm() 테스트 ===")
    vec = [3.0, 4.0, 0.0, -1.0]
    normed = rmsnorm(vec)
    print(f"정규화 전: {vec}")
    print(f"정규화 후: {[round(v, 3) for v in normed]}")

    # ── 종합: 랜덤 가중치로 "다음 문자" 예측 ────────────────────────
    print("\n=== 랜덤 가중치로 다음 문자 예측 ===")

    vocab_size = 27   # a~z (26개) + BOS (1개)
    n_embd = 8        # 임베딩 차원 (작게 설정)

    # 가중치를 랜덤하게 초기화 (학습 전 상태)
    random.seed(42)
    embedding_table = [
        [random.gauss(0, 0.08) for _ in range(n_embd)]
        for _ in range(vocab_size)
    ]
    output_weights = [
        [random.gauss(0, 0.08) for _ in range(n_embd)]
        for _ in range(vocab_size)
    ]

    # "e" (token_id=4) 를 입력으로 넣기
    token_id = 4   # 'e'
    x = embedding_table[token_id]   # 토큰을 벡터로 변환

    # 신경망 연산
    x = rmsnorm(x)                  # 정규화
    logits = linear(x, output_weights)  # 선형 변환 → 각 토큰의 "점수"
    probs = softmax(logits)         # 점수를 확률로 변환

    # 가장 높은 확률의 토큰 찾기
    best_token = probs.index(max(probs))

    uchars = list('abcdefghijklmnopqrstuvwxyz')  # a=0, b=1, ..., z=25
    predicted_char = uchars[best_token] if best_token < 26 else '[BOS]'

    print(f"입력 문자: 'e' (token_id={token_id})")
    print(f"예측된 다음 문자: '{predicted_char}' (token_id={best_token})")
    print(f"예측 확률: {probs[best_token]:.4f} ({probs[best_token]*100:.1f}%)")
    print()
    print("★ 랜덤 가중치이므로 예측이 엉터리입니다. 당연합니다!")
    print("★ 다음 챕터에서 이 가중치를 '학습'시켜 올바른 예측을 만들겠습니다.")
```

---

## 실행 결과 확인

```bash
python ch02_neural_net.py
```

예상 출력:

```
=== linear() 테스트 ===
입력: [1.0, 2.0, 3.0]
가중치:
  [0.1, 0.2, 0.3]
  [0.4, 0.5, 0.6]
출력: [1.4000000000000001, 3.2]

=== softmax() 테스트 ===
입력 logits: [2.0, 1.0, 0.5]
출력 확률:   [0.659, 0.243, 0.098]
확률 합계:   1.000000

=== rmsnorm() 테스트 ===
정규화 전: [3.0, 4.0, 0.0, -1.0]
정규화 후: [1.072, 1.429, 0.0, -0.357]

=== 랜덤 가중치로 다음 문자 예측 ===
입력 문자: 'e' (token_id=4)
예측된 다음 문자: 'q' (token_id=16)
예측 확률: 0.0513 (5.1%)

★ 랜덤 가중치이므로 예측이 엉터리입니다. 당연합니다!
★ 다음 챕터에서 이 가중치를 '학습'시켜 올바른 예측을 만들겠습니다.
```

`e` 다음에 `q`가 나온다고 예측했습니다. 실제로 영어 이름에서 `e` 다음에 `q`가 오는 경우는 거의 없습니다. 가중치가 무작위이기 때문에 예측도 무작위입니다.

---

## 핵심 정리

| 함수 | 역할 | 비유 |
|------|------|------|
| `linear(x, w)` | 입력과 가중치를 곱해 새로운 벡터 생성 | 가중 투표 |
| `softmax(logits)` | 임의 숫자 리스트를 합이 1인 확률로 변환 | 점수를 퍼센트로 |
| `rmsnorm(x)` | 벡터의 크기를 일정 범위로 정규화 | 점수 범위 맞추기 |
| 가중치(weight) | 신경망이 "아는 것"을 저장하는 숫자 | 학습으로 조율되는 다이얼 |

**지금까지 배운 것들의 관계:**

```
[Ch01] 데이터 → 토큰 시퀀스
                    ↓
[Ch02] 토큰 → 임베딩 벡터 → rmsnorm → linear → softmax → 확률
                                                              ↓
                                                  "다음 문자가 a일 확률 30%,
                                                   b일 확률 5%, ..."
```

**어떻게 고치지?**

랜덤 예측을 올바른 예측으로 만들려면, 가중치를 수천 번 조금씩 조정해야 합니다. 이 과정이 **학습(Training)** 이고, 다음 챕터부터 배울 내용입니다.


---
[< 이전: Ch01: 데이터와 토크나이저](ch01-data-tokenizer.md) | [목차](../README.md) | [다음: Ch03: 왜 학습이 되는가? >](ch03-why-learning-works.md)
