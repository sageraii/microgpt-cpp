# Ch03: 왜 학습이 되는가?

## 학습 목표

이 챕터를 마치면 다음을 이해할 수 있습니다.

- "학습"이 수학적으로 무엇을 의미하는지
- 손실 함수(loss)가 왜 필요한지
- 기울기(gradient)가 어떤 역할을 하는지
- 경사하강법으로 파라미터를 어떻게 업데이트하는지
- 간단한 직선 피팅 예제를 손으로 계산할 수 있는지

---

## 1. "학습"이란 무엇인가? — 눈 가리고 산 내려가기

산 정상에 서 있다고 상상해 보세요. 눈을 가리고 있습니다. 목표는 골짜기(가장 낮은 곳)까지 내려가는 것입니다.

눈이 안 보이니 전체 지형을 볼 수 없습니다. 하지만 지금 서 있는 발바닥의 경사는 느낄 수 있습니다. **"오른쪽이 내리막이다"**라는 걸 느끼면, 오른쪽으로 한 걸음 내딛습니다. 그리고 다시 경사를 느끼고, 또 걸음을 내딛습니다. 이것을 반복하면 결국 골짜기에 도달합니다.

AI 학습도 똑같습니다.

- **산의 높이** = 손실(loss): 모델이 얼마나 틀렸는지
- **발바닥의 경사** = 기울기(gradient): 어느 방향으로 파라미터를 바꾸면 loss가 줄어드는지
- **한 걸음** = 파라미터 업데이트: 기울기 방향으로 조금씩 이동
- **골짜기** = 최솟값: 모델이 데이터를 가장 잘 설명하는 상태

---

## 2. 파라미터란 무엇인가?

모델이 "아는 것"은 전부 **파라미터(parameter)**에 담겨 있습니다.

처음에는 파라미터가 완전히 랜덤합니다. 모델이 아무것도 모르는 상태입니다. 학습을 거치면서 파라미터가 조금씩 조정되고, 모델은 점점 데이터의 패턴을 "알게" 됩니다.

`microgpt.py`에서는 이렇게 파라미터를 초기화합니다.

```python
# microgpt.py:80
matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]
```

`random.gauss(0, std)` — 평균 0, 표준편차 0.08인 정규분포에서 랜덤한 숫자를 뽑아 파라미터를 만듭니다. 골짜기를 찾아 떠나는 여정의 시작점이 랜덤한 것입니다.

---

## 3. 손실 함수(Loss Function) — "얼마나 틀렸는지" 점수 매기기

모델이 예측을 하면, 그 예측이 정답과 얼마나 다른지 숫자 하나로 표현해야 합니다. 그 숫자가 **손실(loss)**입니다.

- loss가 **높을수록** = 모델이 많이 틀렸음
- loss가 **낮을수록** = 모델이 잘 맞혔음
- loss = **0** = 완벽한 예측 (현실에서는 거의 불가능)

학습의 목표는 loss를 최대한 낮추는 것입니다.

---

## 4. 크로스 엔트로피 — 언어 모델의 손실 함수

`microgpt.py`는 이름을 생성하는 언어 모델입니다. 각 위치에서 "다음 글자가 무엇일지" 확률로 예측합니다.

예를 들어 모델이 다음 글자 예측을 이렇게 했다고 합시다.

| 글자 | 예측 확률 |
|------|----------|
| a    | 0.7      |
| b    | 0.2      |
| c    | 0.1      |

정답이 `a`라면, 모델이 꽤 잘 예측한 것입니다. 정답이 `c`라면, 많이 틀린 것입니다.

이를 수치로 표현하는 공식이 **크로스 엔트로피**입니다.

```
loss = -log(정답 글자의 예측 확률)
```

구체적인 예를 봅시다.

| 정답 확률 | loss 계산              | loss 값 |
|---------|----------------------|--------|
| 0.9     | -log(0.9) = 0.105    | 낮음 (잘 맞힘) |
| 0.5     | -log(0.5) = 0.693    | 보통   |
| 0.1     | -log(0.1) = 2.303    | 높음 (많이 틀림) |
| 0.01    | -log(0.01) = 4.605   | 매우 높음 |

확률이 높을수록 loss가 낮아집니다. 직관과 잘 맞습니다.

`microgpt.py`에서는 이렇게 계산합니다.

```python
# microgpt.py:167
loss_t = -probs[target_id].log()
```

`probs[target_id]` — 정답 토큰의 예측 확률
`.log()` — 자연로그
`-` — 부호를 뒤집어 양수로 만들기 (log는 0~1 사이에서 음수)

---

## 5. 기울기(Gradient) — "어느 방향으로 가야 loss가 줄어드는가"

경사하강법에서 "발바닥의 경사"에 해당하는 것이 **기울기(gradient)**입니다.

기울기는 이런 의미입니다.

> "이 파라미터를 아주 조금 늘리면 loss가 얼마나 변하는가?"

- 기울기가 **양수** → 파라미터를 늘리면 loss가 늘어남 → 파라미터를 **줄여야** 함
- 기울기가 **음수** → 파라미터를 늘리면 loss가 줄어듦 → 파라미터를 **늘려야** 함
- 기울기가 **0** → 이미 최솟값 근처

---

## 6. 미분이란? — "살짝 건드리면 얼마나 변하나"

기울기를 계산하는 수학적 도구가 **미분**입니다.

비유: 내가 지금 음식 레시피를 시험하고 있다고 합시다. 소금을 1g 넣었을 때 맛이 10점이었습니다. 소금을 1.001g 넣었더니 맛이 10.003점이 되었습니다. 그러면 "소금을 조금 더 넣으면 맛이 좋아진다"는 것을 알 수 있습니다.

수식으로 표현하면:

```
미분(맛, 소금) ≈ (10.003 - 10) / (1.001 - 1) = 0.003 / 0.001 = 3
```

이것이 미분의 기본 아이디어입니다. 파라미터를 아주 조금(h) 바꿨을 때 loss가 얼마나 변하는지를 측정합니다.

---

## 7. 경사하강법 — 한 걸음씩 골짜기로

기울기를 알았으면 파라미터를 업데이트할 수 있습니다.

```
parameter = parameter - learning_rate × gradient
```

- `learning_rate` (학습률): 한 걸음의 크기. 너무 크면 골짜기를 지나쳐 버립니다. 너무 작으면 학습이 느립니다.
- `gradient`: 방향. 기울기의 반대 방향으로 이동합니다.

`microgpt.py`에서는 Adam이라는 개선된 버전을 사용하지만, 기본 원리는 같습니다.

```python
# microgpt.py:181 (단순화한 버전)
p.data -= learning_rate * p.grad
```

---

## 8. 미니 실습 — y = ax + b 직선 피팅

이제 직접 손으로 계산해 봅시다. 단순한 직선 하나를 데이터에 맞추는 예제입니다.

### 문제 설정

데이터 포인트: `(x=1, y=2)`, `(x=2, y=4)`, `(x=3, y=6)`
정답 직선: `y = 2x + 0` (즉, a=2, b=0)
시작값: `a=0, b=0` (아무것도 모르는 상태)

### 손실 함수

```
loss = (예측값 - 정답)² = (ax + b - y)²
```

평균 손실:
```
loss = [(a·1+b-2)² + (a·2+b-4)² + (a·3+b-6)²] / 3
```

### 0단계 초기 상태 확인

a=0, b=0일 때:
```
예측: 0·1+0=0, 0·2+0=0, 0·3+0=0
loss = [(0-2)² + (0-4)² + (0-6)²] / 3
     = [4 + 16 + 36] / 3
     = 56 / 3 ≈ 18.67
```

loss가 매우 높습니다. 아무것도 모르는 상태이니 당연합니다.

### 기울기 계산

미적분을 사용하면 각 파라미터에 대한 기울기는:

```
∂loss/∂a = (2/n) · Σ (ax+b-y) · x
∂loss/∂b = (2/n) · Σ (ax+b-y)
```

a=0, b=0에서 계산하면:
```
∂loss/∂a = (2/3)·[(0-2)·1 + (0-4)·2 + (0-6)·3]
          = (2/3)·[-2 + (-8) + (-18)]
          = (2/3)·(-28) ≈ -18.67

∂loss/∂b = (2/3)·[(0-2) + (0-4) + (0-6)]
          = (2/3)·(-12) = -8.0
```

### 파라미터 업데이트 (learning_rate = 0.05)

```
a = 0 - 0.05 · (-18.67) = 0 + 0.933 = 0.933
b = 0 - 0.05 · (-8.0)   = 0 + 0.4   = 0.4
```

### 1단계 후 loss 확인

```
예측: 0.933·1+0.4=1.333, 0.933·2+0.4=2.267, 0.933·3+0.4=3.2
loss = [(1.333-2)² + (2.267-4)² + (3.2-6)²] / 3
     = [0.444 + 3.001 + 7.84] / 3
     ≈ 3.76
```

18.67에서 3.76으로 줄었습니다. 한 번만 업데이트했는데도 크게 개선되었습니다!

### 10번 반복 후 수렴 과정

아래는 Python으로 동일한 과정을 자동화한 코드입니다. 직접 실행해 보세요.

```python
# 데이터
xs = [1, 2, 3]
ys = [2, 4, 6]

# 초기 파라미터
a = 0.0
b = 0.0
lr = 0.05  # 학습률

print(f"{'step':>4} | {'a':>8} | {'b':>8} | {'loss':>10}")
print("-" * 42)

for step in range(10):
    # 현재 loss 계산
    preds = [a * x + b for x in xs]
    loss = sum((p - y) ** 2 for p, y in zip(preds, ys)) / len(xs)

    # 기울기 계산 (미분)
    n = len(xs)
    grad_a = (2 / n) * sum((a * x + b - y) * x for x, y in zip(xs, ys))
    grad_b = (2 / n) * sum((a * x + b - y)     for x, y in zip(xs, ys))

    # 파라미터 업데이트 (경사하강법)
    a = a - lr * grad_a
    b = b - lr * grad_b

    print(f"{step+1:>4} | {a:>8.4f} | {b:>8.4f} | {loss:>10.4f}")

print(f"\n최종 결과: a={a:.4f}, b={b:.4f}")
print(f"정답:      a=2.0000, b=0.0000")
```

### 실행 결과

```
step |        a |        b |       loss
------------------------------------------
   1 |   0.9333 |   0.4000 |    18.6667
   2 |   1.4756 |   0.5173 |     3.7600
   3 |   1.7389 |   0.4013 |     0.9252
   4 |   1.8705 |   0.2748 |     0.3048
   5 |   1.9317 |   0.1760 |     0.1133
   6 |   1.9626 |   0.1090 |     0.0448
   7 |   1.9800 |   0.0659 |     0.0181
   8 |   1.9891 |   0.0392 |     0.0074
   9 |   1.9942 |   0.0230 |     0.0031
  10 |   1.9970 |   0.0134 |     0.0013

최종 결과: a=1.9970, b=0.0134
정답:      a=2.0000, b=0.0000
```

10번만에 a는 2.0에 가까워지고 loss는 18.67에서 0.001로 급감했습니다. 경사하강법이 실제로 동작한다는 것을 확인했습니다.

---

## 9. 핵심 정리

| 개념 | 한 줄 설명 | 비유 |
|------|-----------|------|
| 파라미터 | 모델이 "아는 것". 처음엔 랜덤, 학습으로 개선 | 레시피의 재료 비율 |
| 손실(loss) | 예측이 얼마나 틀렸는지 나타내는 숫자. 낮을수록 좋음 | 시험 오답 수 |
| 크로스 엔트로피 | 언어 모델의 손실 함수. `-log(정답 확률)` | 확률이 낮은 답을 골랐을 때 벌점 |
| 기울기(gradient) | "이 파라미터를 늘리면 loss가 어떻게 변하는가" | 발바닥으로 느끼는 경사 |
| 경사하강법 | `parameter -= learning_rate × gradient` | 경사 반대 방향으로 한 걸음씩 |
| 학습률 | 한 걸음의 크기. 너무 크면 진동, 너무 작으면 느림 | 보폭 |

### 다음 챕터 예고

이번 챕터에서는 기울기를 **손으로 직접 계산**했습니다. 그런데 `microgpt.py`에는 수십 개의 연산이 복잡하게 연결되어 있습니다. 매번 손으로 미분하는 것은 불가능합니다.

다음 챕터에서는 이 기울기 계산을 **자동으로** 해주는 `Value` 클래스를 살펴봅니다.


> **직접 체험하기** — 시각화 도구에서 Loss 곡선과 학습 과정을 실시간으로 확인할 수 있습니다: [라이브 데모에서 직접 체험](https://sageraii.github.io/microgpt-cpp/#training)

---
[< 이전: Ch02: 신경망 첫걸음](ch02-first-neural-net.md) | [목차](../README.md) | [다음: Ch04: Value와 역전파 >](ch04-value-and-backprop.md)
